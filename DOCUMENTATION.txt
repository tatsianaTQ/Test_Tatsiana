==================================================
DOCUMENTATION DU PROJET - WEEKLY DATA COLLECTOR
==================================================

Ce document détaille le fonctionnement interne, la logique et les points clés du projet d'automatisation de collecte de données.


---------------------------
1. OBJECTIF DU PROJET
---------------------------

Le but est d'automatiser un processus manuel et répétitif :
1.  Lancer plusieurs scripts Python (scrapers) qui naviguent sur un site web pour collecter des données.
2.  Chaque scraper génère un fichier CSV et un fichier Excel.
3.  Une fois tous les scrapers terminés, rassembler tous les fichiers générés dans une seule archive ZIP, nommée avec la date du jour.
4.  Envoyer cette archive par e-mail à une liste de destinataires prédéfinis.
5.  Nettoyer les fichiers sources (CSV/Excel) pour ne conserver que l'archive ZIP et les logs.
6.  Planifier cette tâche pour une exécution hebdomadaire sans intervention manuelle.


---------------------------
2. LOGIQUE DE FONCTIONNEMENT
---------------------------

Le cœur du projet est le script `run.py`. Il agit comme un chef d'orchestre et suit les étapes suivantes :

1.  **Initialisation :**
    - Met en place le système de logging : tous les messages (INFO, ERROR, etc.) sont à la fois affichés dans la console et enregistrés dans un fichier horodaté dans le dossier `/logs`. C'est essentiel pour le débogage des exécutions automatiques.
    - Charge les variables d'environnement depuis le fichier `.env` (identifiants SMTP, destinataires).

2.  **Exécution des Scrapers :**
    - Le script parcourt la liste `SCRIPTS_TO_RUN` définie au début du fichier.
    - Pour chaque scraper, il utilise le module `subprocess` de Python pour le lancer dans un processus séparé. Cette méthode est robuste car elle isole chaque scraper. Si l'un d'eux échoue, le processus principal peut le détecter et arrêter l'exécution globale.
    - L'option `-m` (ex: `python -m src.scrapers.1_page_acceuil...`) est utilisée pour que Python traite les scripts comme des modules, ce qui résout les problèmes d'imports relatifs (ex: `from src.common...`).

3.  **Création de l'Archive ZIP :**
    - Si tous les scrapers se sont terminés avec succès, le script recherche dans le dossier `/output` tous les fichiers `.csv` et `.xlsx` contenant la date du jour dans leur nom.
    - Il crée une archive ZIP (`rapport_hebdomadaire_AAAA-MM-JJ.zip`) et y ajoute tous les fichiers trouvés.

4.  **Envoi de l'E-mail :**
    - Le script se connecte au serveur SMTP spécifié dans le fichier `.env` (ici, SendGrid).
    - Il construit un e-mail avec un sujet, un corps de texte, et attache l'archive ZIP.
    - Il gère plusieurs destinataires principaux (`EMAIL_TO`) et en copie (`EMAIL_CC`), en les séparant par des virgules dans le fichier `.env`.

5.  **Nettoyage :**
    - Si l'e-mail a été envoyé avec succès, le script supprime les fichiers CSV et Excel qui ont été archivés, afin de garder le dossier `/output` propre. Seul le fichier ZIP est conservé comme preuve de l'exécution.

6.  **Fin du Processus :**
    - Le script enregistre la durée totale de l'exécution et se termine.



